# Simplified LocalAI setup with a lightweight model
# This uses a smaller, faster model suitable for local development
services:
  localai:
    image: quay.io/go-skynet/local-ai:latest-aio-cpu
    container_name: empathic-proxy-localai
    ports:
      - "8081:8080"
    environment:
      - MODELS_PATH=/models
      - THREADS=4
      - CONTEXT_SIZE=2048
      - DEBUG=true
      - PRELOAD_MODELS=true
      - GALLERIES=[{"name":"model-gallery","url":"https://raw.githubusercontent.com/go-skynet/model-gallery/main"}]
    volumes:
      - ./models:/models
      - ./localai-config:/config
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped

